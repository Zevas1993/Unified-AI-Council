{
  "name": "unified-ai-council",
  "displayName": "Unified AI Council",
  "description": "One chat, three AI coding CLIs (Codex + Claude Code + Gemini) orchestrated as a council inside VS Code.",
  "version": "0.1.0",
  "publisher": "Zevas1993",
  "license": "MIT",
  "repository": {
    "type": "git",
    "url": "https://github.com/Zevas1993/Unified-AI-Council.git"
  },
  "homepage": "https://github.com/Zevas1993/Unified-AI-Council#readme",
  "bugs": {
    "url": "https://github.com/Zevas1993/Unified-AI-Council/issues"
  },
  "keywords": [
    "ai",
    "codex",
    "claude",
    "gemini",
    "council",
    "orchestrator",
    "coding-assistant",
    "multi-agent",
    "wsl"
  ],
  "engines": {
    "vscode": "^1.92.0",
    "node": ">=18.0.0"
  },
  "categories": [
    "AI",
    "Machine Learning",
    "Other"
  ],
  "activationEvents": [
    "onView:unifiedAiCouncil.sidebar",
    "onCommand:unifiedAiCouncil.open",
    "onCommand:unifiedAiCouncil.setup",
    "onCommand:unifiedAiCouncil.resetMemory",
    "onCommand:unifiedAiCouncil.validateConfig"
  ],
  "main": "./dist/extension.js",
  "contributes": {
    "viewsContainers": {
      "activitybar": [
        {
          "id": "unifiedAiCouncil",
          "title": "Unified AI Council",
          "icon": "media/icon.svg"
        }
      ]
    },
    "views": {
      "unifiedAiCouncil": [
        {
          "id": "unifiedAiCouncil.sidebar",
          "name": "Council Chat"
        }
      ]
    },
    "commands": [
      {
        "command": "unifiedAiCouncil.open",
        "title": "Unified AI Council: Open"
      },
      {
        "command": "unifiedAiCouncil.setup",
        "title": "Unified AI Council: Setup OAuth (WSL)"
      },
      {
        "command": "unifiedAiCouncil.resetMemory",
        "title": "Unified AI Council: Reset Project Memory"
      },
      {
        "command": "unifiedAiCouncil.validateConfig",
        "title": "Unified AI Council: Validate Configuration"
      }
    ],
    "configuration": {
      "title": "Unified AI Council",
      "properties": {
        "unifiedAiCouncil.wsl.distro": {
          "type": "string",
          "default": "",
          "description": "Optional WSL distro name (e.g., Ubuntu). Leave blank to use default."
        },
        "unifiedAiCouncil.wsl.shell": {
          "type": "string",
          "default": "bash",
          "description": "WSL shell to use (bash recommended)."
        },
        "unifiedAiCouncil.cli.codex.command": {
          "type": "string",
          "default": "codex",
          "description": "Codex CLI command inside WSL."
        },
        "unifiedAiCouncil.cli.claude.command": {
          "type": "string",
          "default": "claude",
          "description": "Claude Code CLI command inside WSL."
        },
        "unifiedAiCouncil.cli.gemini.command": {
          "type": "string",
          "default": "gemini",
          "description": "Gemini CLI command inside WSL."
        },
        "unifiedAiCouncil.cli.timeoutMs": {
          "type": "number",
          "default": 180000,
          "description": "Timeout per council member run (ms)."
        },
        "unifiedAiCouncil.memory.maxEntries": {
          "type": "number",
          "default": 500,
          "description": "Max memory entries stored per workspace."
        },
        "unifiedAiCouncil.orchestrator.engine": {
          "type": "string",
          "default": "embedded",
          "enum": [
            "embedded",
            "nano",
            "ollama"
          ],
          "description": "Orchestrator engine. 'embedded' uses Transformers.js with a local ONNX model. 'nano' uses heuristic templates. 'ollama' uses a user-local Ollama server."
        },
        "unifiedAiCouncil.embedded.enabled": {
          "type": "boolean",
          "default": true,
          "description": "Enable the embedded nano LLM for prompt shaping and synthesis."
        },
        "unifiedAiCouncil.embedded.modelId": {
          "type": "string",
          "default": "onnx-community/Qwen2.5-Coder-0.5B-Instruct",
          "description": "Hugging Face model ID for the embedded LLM (must have ONNX weights)."
        },
        "unifiedAiCouncil.embedded.maxTokens": {
          "type": "number",
          "default": 256,
          "description": "Maximum tokens to generate with the embedded LLM."
        },
        "unifiedAiCouncil.embedded.temperature": {
          "type": "number",
          "default": 0.3,
          "description": "Sampling temperature for the embedded LLM. Lower = more deterministic."
        },
        "unifiedAiCouncil.orchestrator.ollama.baseUrl": {
          "type": "string",
          "default": "http://localhost:11434",
          "description": "Ollama base URL used when orchestrator.engine = 'ollama'."
        },
        "unifiedAiCouncil.orchestrator.ollama.model": {
          "type": "string",
          "default": "llama3.2:3b",
          "description": "Ollama model name used when orchestrator.engine = 'ollama'."
        },
        "unifiedAiCouncil.orchestrator.ollama.temperature": {
          "type": "number",
          "default": 0.2,
          "description": "Sampling temperature for the local synthesizer. Lower = more deterministic."
        },
        "unifiedAiCouncil.orchestrator.ollama.topP": {
          "type": "number",
          "default": 0.95,
          "description": "Top-p nucleus sampling for the local synthesizer."
        },
        "unifiedAiCouncil.orchestrator.ollama.maxTokens": {
          "type": "number",
          "default": 900,
          "description": "Max tokens requested from the local synthesizer."
        },
        "unifiedAiCouncil.orchestrator.maxContextChars": {
          "type": "number",
          "default": 14000,
          "description": "Maximum context passed to each CLI (characters)."
        }
      }
    }
  },
  "scripts": {
    "compile": "tsc -p .",
    "watch": "tsc -watch -p .",
    "lint": "eslint src",
    "test": "vitest run",
    "package": "vsce package",
    "prepublishOnly": "npm run compile"
  },
  "devDependencies": {
    "@eslint/js": "^9.39.2",
    "@types/node": "^20.14.10",
    "@types/vscode": "^1.92.0",
    "eslint": "^9.8.0",
    "typescript": "^5.5.4",
    "typescript-eslint": "^8.51.0",
    "vitest": "^2.1.8",
    "vsce": "^2.15.0"
  },
  "dependencies": {
    "@huggingface/transformers": "^3.8.1"
  }
}
